\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:
%
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{alltt}
\renewcommand{\ttdefault}{txtt}

\usepackage{amsmath}
\usepackage{mathtools}
\everymath{\displaystyle}
\usepackage{xspace}

\newcommand{\CEU}{\textsc{C\'{e}u}\xspace}
\newcommand{\code}[1] {{\small{\texttt{#1}}}}
\newcommand{\DOFIN}{\code{do-finally}\xspace}
\newcommand{\FIN}{\code{finally}\xspace}

\newcommand{\ST}{\xrightarrow[~i~]{}}
\newcommand{\BT}{\xRightarrow[(i,E)]{}}

\newcommand{\1}{\;}
\newcommand{\2}{\;\;}
\newcommand{\3}{\;\;\;}
\newcommand{\5}{\;\;\;\;\;}
\newcommand{\ten}{\5\5}
\newcommand{\twenty}{\ten\ten}

\newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}

\usepackage{enumitem}
\setlist{nolistsep}

\begin{document}

\conferenceinfo{PLDI '13}{date, City.} \copyrightyear{2005} \copyrightdata{[to 
be supplied]} 

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

%\title{Embedded Development without Buts:}
%\subtitle{A Full-featured Language Design for Constrained Embedded Systems}
\title{...}
\subtitle{...}

\authorinfo
    {Francisco Sant'Anna \and Noemi Rodriguez \and Roberto Ierusalimschy}
    {Departamento de Inform\'atica --- PUC-Rio, Brasil}
    {\{fsantanna,noemi,roberto\}@inf.puc-rio.br}

\maketitle

\begin{abstract}
\begin{comment}
The particular requirements of embedded systems impose a challenge to language 
designers which must valuate a tradeoff between a reliable and comprehensive 
set of features and limitations from platforms.
On the one hand, programmers demand effective abstractions concerning aspects 
such as concurrency, componentization, and resource management, ideally with 
unrestricted access to low-level functionality.
On the other hand, embedded platforms impose severe constraints on CPU, memory 
and battery to applications demanding correct and predictable behavior.

%unforeseen, unexpected


TODO: ref surveys

TODO
Our design shows that it is possible to bring high-level functionality 
unthoutgh in a general purpose ES language, while being compliant with the 
safety and constrained requirements of embedded systems.

TODO: falar como implementar um buffer, vec+safeana+macros

TODO: orthogonal preemption, falar e citar na semantica
the fact that they are killed while blocked, deterministic and ortoghonal
impossible in asynchronous formalisms

In this work, we present the design of \CEU,

The design decisions behind \CEU prioritize safety with the tractable 
synchronous execution model
In the end we offer a language with good balance between

The design of programming languages for embedded systems
The more features, more memory
The more flexibility, less security

good balance

pilha restrita
memoria bounded
compliant with the safety and constrained requirements of embedded systems, 

when a language provides a XXX concurrency model, they are restricted

we present a concurrency model that introduces a novel semantics for internal 
events
enables finalizer, exception handling, which are found in high-level language 
and are never considered in constrained contexts.
furthermore, they lack advanced xxx such as

As orthogonal features, \CEU also provides first-class wall-clock time, 
asynchronous execution for time consuming tasks, and simulation of programs in 
the language itself.
Specially in the context of embedded systems, these features are essential, 
given that time is the most common input, and simulation is a requirement in
cross-compiling environments.

== concurrency
- deterministic
- bounded

== componentization
- compositions
- macros (bad for ROM, good for RAM)

== resource
- no alloc
    - ok, we have sequential and scopes
    - 1x1 mapping
- no WC
    - only for dyn data-structures
    - finalizers (destructors)
        - two kinds:
            - for GC
            - for control structs
                - ceu provides this
                - substitutes the other?
- wclocks
- reactivity

lock-free concurrency

All examples are reproduceable through a virtual machine.

Finally, size and safety
still safe
    - detects conc acces
    - all memory is known at compile time
        - stackless
        - no dynamic runtime
        - the static analysis also detects the exact mem size
low-level

- Requirements
    - reactivity
        - speed of the environment
        - real-time
    - memory
    - reliability
        - deterministic
    - cross-development
        - simulation
    - flexibility?


- disallows shared memory
- offer DSLs that hide the platform
- low-level hiding which is not hidden in \CEU (e.g., 

or safety warranties
    - unbounded execution
    - arbitrary semantics
    - ignores nondeterminism


good balance between low-level

In Section~\ref{}, we show how to implement typical patterns in embedded 
systems.
We also discuss how we can derive advanced constructs from full-featured 
programming languages, such as exception handling, finalizers

- Features
    - first-class timers
    - shared-memory concurrency
    - finalizers
    - exceptions
    - sequential reactivity
    - dataflow
        - mutual dependency
    - formal (simple?) semantics

- Non-features
    - static
    - exponential analysis

the static analysis also detects the exact mem size
it is impossible to overflow timers/spawns
looks like dynamic but it is not
optionally, defensive (just count the number)

finalizers are important for dynamic support

static
no virtualization of devices
1x1

\CEU still has the limitation of not creating new lines of execution at runtime
They are all known in advance to properly calculate the amount of memory
also, dynamic would require a more complex static analysis
Note that dynamically allocating threads is not common in ES (chibi, what 
else?)

Currently, CEUs not intended for use outside ES context
malloc/free is not enough, control structures should also be dynamic
implementation of objects

Mutual dependency

- it is impossible to write a cyclic dep. in ceu
- howerver high-level mutual dep is possible
- in other languages, they cannot prove that a cycle stablizes
- in ceu all cycles compute a single dep., independently if they stablize or 
  not
- other languages use a dubious/questionable trick with a unclear semantics
    - would break static analysis
    - what does it exactly mean? you cant say in which state it will be after N 
      events
    - lost synchronism
\end{comment}

\end{abstract}

%\category{CR-number}{subcategory}{third-level}
\category{D.3.1}{Programming Languages}{Formal Definitions and Theory}
\category{D.3.3}{Programming Languages}{Language Constructs and Features}

\terms{Design, Languages, Reliability}

\keywords{Concurrency, Determinism, Embedded Systems, Safety, Static Analysis, 
Synchronous}

\section{Introduction}

Embedded systems are usually designed with safety and real-time requirements 
under highly constrained hardware platforms.
At the same time, developers demand effective programming abstractions, ideally 
with unrestricted access to low-level functionality.

These particularities impose a challenge to embedded-language designers, who 
must provide a comprehensive set of features targeting platforms that require 
correct and predicable behavior but offer limited CPU, memory, and battery.
As a consequence, embedded languages either lack functionality or fail to offer 
a small and reliable programming environment.

This dilemma is notably evident in multithreading support for embedded systems, 
which implies a considerable overhead for per-thread 
stacks~\cite{wsn.protothreads} and synchronization primitives.
%, which provides high-level structured programming for reactive applications 
%and contrast with event-driven programming.
Also, preemptive scheduling is a potential source of 
hazards~\cite{sync_async.threadsproblems}.
Alternative designs enforce cooperative scheduling to eliminate race 
conditions, but potentialize unbounded execution, breaking real-time 
responsiveness in programs~\cite{wsn.comparison}.
Therefore, language designers have basically three options:
not providing threads at all~\cite{wsn.nesc}, affecting the productivity of 
programmers;
providing a restricted alternative, such as disallowing locals in 
threads~\cite{wsn.protothreads};
or preserving full support, but offering coarsed-grained concurrency 
only~\cite{wsn.mantisos}.

In this work, we present the design of \CEU%
\footnote{C\'eu is the Portuguese word for \emph{sky}.},
a reactive programming language that provides a reliable yet powerful 
programming environment for embedded systems.
\CEU is based on Esterel~\cite{esterel.ieee91} and follows the synchronous 
execution model~\cite{rp.twelve}, which enforces a disciplined step-by-step 
execution that enables lock-free concurrency.
\CEU distinguishes from Esterel in basically two characteristics:

\begin{itemize}
\item Programs can only react to a \emph{single} external event at a time.  
\item Internal events follow a \emph{stacked} execution policy (like function 
calls in typical programming languages).
\end{itemize}

These design decisions are fundamental to introduce new functionalities to 
\CEU:

\begin{itemize}
\item The uniqueness of external events enables a static analysis that provides 
safe shared-memory concurrency.
\item The stacked execution of internal events permits to derive many advanced 
control mechanisms, such as \DOFIN constructs, exception handling, and dataflow 
programming.
\end{itemize}

In our discussion, shared memory concerns not only variables, but also 
low-level acesses that ultimately use shared resources in the underlying 
platform (e.g., memory-mapped ports for I/O).

The stacked execution for internal events introduces support for a restricted 
non-recursive form of subroutines, resulting in memory-bounded programs that 
preclude stack overflows.

The proposed new functionalities are compliant with the safety and constrained 
requirements of embedded systems and, arguably, do not dramatically reduce the 
expressiveness of the language.
As a limitation of the synchronous model, computations that run in unbounded 
time do not fit the zero-delay hypothesis~\cite{rp.hypothesis}, and cannot be 
elegantly implemented in \CEU.

The implementation of \CEU offers fine-grained concurrency for highly 
constrained platforms~\cite{ceu.tr}.
For instance, the current memory footprint under Arduino~\cite{arduino.cc} is 
around 2 Kbytes of ROM and 50 bytes of RAM.
A program with sixteen lines of execution (with empty bodies) that synchronize 
on termination incur extra 270 bytes of ROM and 60 bytes of RAM.

The rest of the paper is organized as follows:
Section~\ref{sec.ceu} briefly introduces \CEU and describes it formally through 
an operational semantics.
%focusing on its control-intensive and reactive nature.
Section~\ref{sec.safety} demonstrates how the language can ensure deterministic 
behavior for reactions to the environment using bounded resources (i.e., memory 
and CPU).
Section~\ref{sec.adv} shows how to implement some advanced control-flow 
mechanisms on top of the simpler semantics of internal events.
Section~\ref{sec.related} compares \CEU to related work following both the 
synchronous and asynchronous execution models.
Section~\ref{sec.conclusion} concludes the paper and makes final remarks.

%\newpage
\section{The programming language \CEU}
\label{sec.ceu}

\CEU is a synchronous reactive language with support for multiple lines of 
execution known as \emph{trails}.
By reactive, we mean that programs are stimulated by the environment through 
input events that are broadcast to all active trails.
By synchronous, we mean that any trail at any given time is either reacting to 
the current event or is blocked awaiting another event;
in other words, trails are always synchronized at the current (and single) 
event.

As an introductory example, the program in Figure~\ref{lst:ceu:1} counts the 
difference in clicks between buttons \code{BT1} and \code{BT2} (represented as 
external input events), terminating when the number of occurrences of 
\code{BT2} is higher.
The program is structured with three trails in parallel to illustrate the 
concurrent and reactive nature of \CEU.
The first and second trails react, respectively, to buttons \code{BT1} and 
\code{BT2} in a loop, while the third trail reacts to internal event 
\code{clicked}.

\begin{figure}[t]
%\rule{8.5cm}{0.37pt}
{\small
\begin{verbatim}
 1:  input void BT1, BT2;   // external input events
 2:  event int clicked;     // an internal event
 3:  par/or do
 4:     loop do             // 1st trail
 5:        await BT1;
 6:        emit clicked(1);
 7:     end
 8:  with
 9:     loop do             // 2nd trail
10:        await BT2;
11:        emit clicked(-1);
12:     end
13:  with
14:     int count = 0;      // 3rd trail
15:     loop do
16:        int v = await clicked;
17:        count = count + v;
18:        _printf("BT1 - BT2 = %d\n", count);
19:        if count < 0 then
20:            break;
21:        end
22:     end
23:  end
\end{verbatim}
}
\caption{ A concurrent program in \CEU.
\label{lst:ceu:1}
}
\end{figure}

Lines 1-2 declare the events used in the program.
An event declaration includes the type of value the occurring event carries.
For instance, the two buttons are notify-only external input events (carrying 
no values), while \code{clicked} is an internal event that holds integer 
values.

The \code{par/or} construct spawns three trails in parallel (lines 3-23).
The loops in the first and second trails (lines 4-7 and 9-12) continuously wait 
for the referred buttons and notify their occurrences through the 
\code{clicked} event.
The third trail (lines 14-22) awakes whenever \code{clicked} is emitted to 
update the difference of occurrences held in local variable \code{count}, 
printing it on screen%
\footnote{
Symbols defined externally in $C$, such as \code{printf}, must be prefixed with 
an underscore to be used in \CEU.
}, and breaking the loop when it is negative.

Given the synchronous execution model of \CEU, the first and second trails 
never execute concurrently because they react to different external events.
Hence, emits to event \code{clicked} in the two trails are race free.
%We discuss determinism in Section~\ref{sec.safety.det}.

A \code{par/or} composition rejoins when any of its trails terminates.
Hence, the termination of the third trail causes the termination of the 
program.
\CEU also supports \code{par/and} compositions, which rejoin when \emph{all} of 
its trails terminate.
The use of trails in parallel allows programs to wait for multiple events while 
keeping context information, such as locals and the program 
counter~\cite{sync_async.cooperative}.

The conjunction of parallelism with typical imperative primitives provides 
structured reactive programming and help on developing applications more 
concisely.
In particular, compositions of sequences, conditionals, loops, and parallelism 
can be used to implement typical patterns found in embedded systems, as we 
discussed in previous work~\cite{ceu.sac}.

One of the particularities of \CEU is on how internal and external events 
behave differently:

\begin{itemize}
\item External events can be emitted only by the environment, while internal 
events only by the program.
\item A single external event can be active at a time, while multiple internal 
events can coexist.
\item External events are handled in queue, while internal events follow a 
stacked execution policy.
% (like subroutine calls in typical programming languages).
\end{itemize}

The stacked execution policy for internal events is a fundamental design 
decision from which many advanced control mechanisms can be derived, as 
presented in Section~\ref{sec.adv}.
To exemplify the stacked behavior, whenever the \code{emit} in line 11 of 
Figure~\ref{lst:ceu:1} executes, its continuation (lines 12,9,10) is delayed 
until the awaken trail in line 16 completely reacts, either breaking the loop 
(line 20) or awaiting again (line 16).

Note that both internal and external events are unbuffered, i.e., at the moment 
an event occurs, only the trails previously waiting for it that will react.

In this work, we focus on a formal description of \CEU that allows us to prove 
some safety properties for programs, such as bounded and deterministic 
execution.
For an extensive and informal presentation of \CEU, refer to the technical 
report~\cite{ceu.tr}.

%\subsection{Operational semantics}
%\textbf{Formalization}
%\label{sec.sem}
\input{formal.tex}

%\newpage
\section{Safety warranties}
\label{sec.safety}

A primeval goal of \CEU is to ensure a reliable execution for shared-memory 
programs.
In this section, we demonstrate how \CEU can ensure at compile time
that reaction chains execute deterministically while using bounded memory and 
time.

%TODO
%Note that relies on the static nature of \CEU, which cannot create

\subsection{Bounded execution}
\label{sec.safety.bounded}

Reactions to the environment should run in bounded time to guarantee that 
programs are responsive and can handle upcoming input events.
Similarly to Esterel~\cite{esterel.ieee91}, \CEU requires that each possible 
path in a loop body contains at least one \code{await} or \code{break} 
statement, thus ensuring that loops never run in unbounded time.

Consider the examples that follow:

{\small
\begin{verbatim}
    loop do                     loop do
        if v > 1000 then            if v > 1000 then
            break;                      break;
        end                         else
        v = _f(v);                      await 1s;
    end                             end
                                    v = _f(v);
                                end
\end{verbatim}
}

The first example is refused at compile time, because the \code{if} true branch 
may never execute, resulting in a \emph{tight loop} (i.e., an infinite loop 
that does not await).
The second variation is accepted, because for every iteration, the loop either 
breaks or awaits.

%%%%%%%%%

Given that programs with tight loops are refused at compile time, we can now 
prove that a reaction chain always executes in bounded time,
i.e, that the presented semantics always make a finite number of derivations 
and reaches the terminating conditions of Section~\ref{sec.sem}.

As an auxiliary lemma, the small-step semantics necessarily reaches a state in 
which all trails are either blocked or terminated, because, except for 
\textbf{loop-expd} (which expands code), all other small-step rules naturally 
advance to the $isBlocked$ condition.
For loop, the compile-time restriction ensures that all trails inside a loop 
either blocks or breaks (reducing the whole expansion to a $nop$ via rule 
\textbf{loop-brk}).

We still need to prove that interleaving big steps and small steps does not 
lead to a runtime cycle involving internal emits and awaits in parallel.
The intuitive argument is that the stacked execution for internal events cannot 
create zero-delay mutual dependency among trails.
The proof uses induction on the depth level of a small step sequence:

\begin{itemize}
\item Inductive hypothesis: for a small step sequence at any depth, a trail 
that reaches an $await$ can only awake from a deeper small step sequence.
\item Base case: an $await$ at deepest depth level cannot awake.
\end{itemize}

To prove the inductive step, a program that reaches an $await$ cannot awake 
from an $emit$ during the current or any previous depth:
an $emit$ at current depth is delayed to a deeper depth (small-step rule 
\textbf{emit});
an $emit$ in any previous depth has already been consumed ($bcast$ takes the 
union of all emits in a big step).

The max number of depth levels must be finite, as only $emit$ statements create 
new depth levels and programs have finite emits.

%%%%%%%%%

Enforcing bounded execution makes \CEU inappropriate for algorithmic-intensive 
applications that require unrestricted loops (e.g.,  cryptography, image 
processing).
However, \CEU is designed for real-time control-intensive applications and we 
believe this is a reasonable price to pay in order to achieve higher 
reliability.

Note that \CEU does not extend the bounded execution analysis for $C$ function 
calls, which are left as responsibility for the programmer.
% TODO: for bad or for good

\subsection{Deterministic execution}
\label{sec.safety.det}

%Nondeterministic execution a big source of software bugs, making concurrency 
%unpredictable
%Determinism is usually a desired safety property for programs, making 
%concurrency predictable and easier to debug.
%\CEU performs a compile-time analysis in order to detect nondeterminism in 
%programs.

Providing deterministic schedulers is a selling point of many concurrent 
designs.
For instance, event-driven systems usually employ a \emph{FIFO} policy for 
event handlers, while in cooperative multithreading the programmer himself 
determinates an order of execution among tasks.
Even systems with preemptive multithreading can offer deterministic execution 
for programs~\cite{async.kendo}.
%However, these designs say nothing about determinism in the 
%\emph{specification} of programs, relying only on implementation subtleties to 
%provide predictable execution for programs.

As discussed in Section~\ref{sec.sem.small}, the small-step semantic rules for 
parallel compositions do not specify the exact order in which trails execute, 
leading to nondeterminism in \CEU.
Note that a slight modification to rules \textbf{and-2} and \textbf{or-2} can 
force one trail to execute before any advance on the other, thus enforcing a 
deterministic policy for the scheduler.
However, we believe that any arbitrary order should be avoided, because an 
apparently innocuous reordering of trails modifies the semantics of the 
program.

\CEU takes take a different approach and provides deterministic execution, 
regardless of its nondeterministic scheduler.
At compile time, we run a symbolic interpretation of the program that creates 
an acyclic graph of all $mem$ operations that execute in a reaction chain.
Then, if any two $mem$ operations access the same memory area and are not 
ancestor of one another, then the program is nondeterministic.
The execution is repeated for every possible reaction chain the program can 
reach.

As an example, consider the program and corresponding reaction graphs in 
Figure~\ref{fig:det}.
The algorithm detects that in the reaction chain for the sixth occurrence of 
input event $A$, variable \code{v} is assigned in parallel paths, what 
qualifies a nondeterministic program that is refused at compile time.

\begin{figure}[t]
%\rule{8.5cm}{0.37pt}
\begin{minipage}[c]{0.40\linewidth}
{\small
\begin{verbatim}
 input void A;
 int v;
 par/and do
   loop do
     await A; // A11
     await A; // A12
     v = 1;
   end
 with
   loop do
     await A; // A21
     await A; // A22
     await A; // A23
     v = 2;
   end
 end
\end{verbatim}
}
\end{minipage}
%
\hspace{0.0cm}
%
\begin{minipage}[c]{0.60\linewidth}
\centering
\includegraphics[width=\textwidth]{dfa.png}
\end{minipage}
\caption{ A nondeterministic program in \CEU.
\label{fig:det}
}
\end{figure}

\begin{comment}
The algorithm works as follows:
TODO:

The graph is created as follows:
- par forks/rejoins
- terminates given previous section
- par obriga ambos a andarem, em caso de break tb
- acyclic because of bounded (may repeat node contents)

- if duplicates

The static nature is important
finite number of awaits

In order to prove the effectiveness of the proposed algorithm, we XXX the 
theorems that follow:

\begin{enumerate}
%\item \emph{The reaction graph is a graph.}
%(no cycles)
\item \emph{A reaction graph is unique, regardless of the nondeterministic 
semantics.}
\item \emph{The number of possible reactions in a program is finite.}  
%(mem and final state of awaits)
%\item ifs
%(exponential)
\end{enumerate}

\end{comment}


Unfortunately, the described algorithm is exponential in the number of 
conditionals and parallel trails in a program.
Even so, it is applicable in many situations:

% TODO: embarrassing ref

\begin{itemize}
\item Embedded programs are usually small, not being affected by the 
exponential growth.
\item Many programs are safety-critical and must provide as much warranties as 
    possible.
\item The algorithm is parallelizable, given that reaction chains do not depend 
    on each other.
\item The development phase \emph{per se} does not require safety warranties, 
reducing considerably the number of times the algorithm has to be executed.
\end{itemize}

%Regarding the last item, note that the \CEU runtime can always fall back to 
%the alternative deterministic scheduler to preserve, at least, race-free 
%execution.

% TODO: fazer novos testes, muito estranho estar demorando tanto
% ver c/ versao antiga

%Our experience shows that the analysis is indeed practical.
%We have been using \CEU with Wireless Sensor Networks and applications in the 
%order of 500 lines of code and 25 Kbytes of ROM compile instanltly in a 
%everyday notebook almost instantly.

An orthogonal problem to building reaction graphs is to classify $mem$ 
operations that can be safely executed in parallel paths, avoiding false 
positives in the analysis.
For instance, $mem$ operations that acesses different variables can obviously 
execute concurrently.
However, remember from Section~\ref{sec.sem.syntax} that the $mem$ primitive 
represents not only read \& write access to variables, but also $C$ function 
calls.
Moreover, \CEU also supports pointers, which are required for low-level 
manipulation (e.g., accessing buffers from device drivers).

\CEU enforces a default policy for $mem$ operations as follows:
If a variable is written in a path, then a path in parallel cannot read or 
write to that variable, nor dereference a pointer of that variable type.
An analogous policy is applied for pointers vs variables and pointers vs 
pointers.
Any two $C$ calls cannot appear in parallel paths, as \CEU has no knowledge 
about their side effects.
Also, passing variables as parameters count as read access to them, while 
pointers count as write access to those types (because functions may 
dereference and assign to them).

This policy may still yield some false positives on the analysis.
For instance, the rule for $C$ calls is particularly restrictive, as many 
functions can be safely called concurrently.
Therefore, \CEU supports syntactic annotations that the programmer can use to 
relax the policy explicitly:

\begin{itemize}
\item The \code{pure} modifier declares $C$ functions that do not perform side 
      effects, being allowed to be called concurrently with any other function 
in the program.
\item The \code{deterministic} modifier declares pairs of variables and 
      functions that do not affect each other, being allowed to be used 
concurrently.
\end{itemize}

The following code illustrates \CEU annotations:

{\small
\begin{verbatim}
  pure  _abs;         // 'abs' is side-effect free
  deterministic _led1On with _led2On;
                      // 'led1On' vs 'led2On' is ok
  int*  buf1, buf2;
  det   buf1, buf2;   // point to different memory areas
\end{verbatim}
}

\subsection{Bounded memory}
\label{sec.safety.mem}

\CEU favors a fine-grained use of trails, being common to use trails that await 
a single event.
For this reason, \CEU does not allocate per-trail stacks; instead, all locals 
reside in fixed memory slots held in a static one-dimension vector.
Locals for trails in parallel must coexist in memory, while statements in 
sequence can share space.
% whose size is maximum the program uses at a given time.
%A given position in the vector may hold different data (with variable sizes) 
%during runtime.

The memory for programs in \CEU can be precisely calculated, given that they 
are defined as hierarchies of control-flow statements with explicit forks and 
joins for trails.
This contrasts with threads, which are defined detached from the program 
hierarchy (e.g., a function defined in separate), and requires programmer 
bookkeeping (e.g. creation, synchronization, etc.) that hinders automatic 
memory prediction and management.

\begin{comment}
We extend our basic syntax of Section~\ref{sec.sem.syntax} with local blocks in 
the form $\{~(n)~p~\}$ (i.e., executes $p$ reserving $n$ bytes for locals), and 
derive a formula to calculate the total amount of memory in a program, as 
follows:

{\small
\begin{align*}
tot(~\{~(n)~p~\}~)          &= n + tot(p)         \\
tot(~if~mem~then~p~else~q~) &= MAX(tot(p),tot(q)) \\
tot(~p~;~q~)                &= MAX(tot(p),tot(q)) \\
tot(~loop~p~)               &= tot(p)             \\
tot(~p~and~q~)              &= tot(p) + tot(q)    \\
tot(~p~or~q~)               &= tot(p) + tot(q)    \\
tot(~*~)                    &= 0~~//~(other~stmts)
\end{align*}
}
\end{comment}

Another concern regarding memory consumption is the runtime stack for internal 
events.
Note that during runtime, a trail can only occupy one position in the stack, 
given that an emit pauses the trail until the stack unwinds.
Hence, in the worst case, the runtime stack size is the maximum number of 
trails in parallel containing an \code{emit} statement, which is also trivially 
calculated from the program text.

Besides $C$ calls, which are not under control of \CEU, the other possible 
point of failure regarding memory consumption is the queue for external events.
High-frequency external events may fill up the queue before the program can 
react to them, even with the guaranteed bounded execution.
For projects that must deal with event bursts, \CEU delegates the queue 
management to the underlying system, which can provide its own policy for 
adjusting the queue size, prioritizing events, or signaling the program about 
overflows (e.g., through a custom event).

%\newpage
\section{Advanced control mechanisms}
\label{sec.adv}

In this section we explore the stacked execution semantics for internal events 
in \CEU, demonstrating how it enables many advanced control-flow mechanisms 
without requiring new primitives in the language.

Although the described mechanisms involve thoughtful techniques, keep in mind 
that they can be easily abstracted with compile-time macros%
\footnote{Our programs in \CEU make extensive use of the \emph{m4} 
preprocessor.},
taking advantage of the structured style of \CEU.
As an exception, the \DOFIN construct requires slight additions to the program 
tree and we present it with a dedicated syntax.

\subsection{Subroutines}
\label{sec.adv.sub}

Internal events introduce support in \CEU for a limited form of subroutines, as 
illustrated in Figure~\ref{lst:subs}.a and depicted as follows:

\begin{itemize}
\item A subroutine is represented as a loop that awaits an identifying event 
(e.g., \code{await f}).
\item A subroutine is called in a parallel trail through an emit on the 
corresponding event (e.g., \code{emit f}).
\item The parameter of a subroutine is the type of its corresponding event 
(e.g., \code{event int f}).
\end{itemize}

In the example, the second trail invokes \code{emit f(1)} to ``call'' 
subroutine \code{f}.
Given the stacked execution, the calling trail pauses and awakes the subroutine 
in the first trail.
The subroutine executes its body, loops, and awaits event \code{f} to be called 
again.
Then, the second trail resumes and calls \code{f} in sequence, repeating the 
behavior just described.
In the examples, we assume the subroutine bodies represented as \code{...} do 
not contain \code{await} statements.

The other examples in Figure~\ref{lst:subs} show how this form of subroutines 
is non-recursive (recursive calls have no effect), and can only run a single 
instance at a time (i.e., calls to running subroutines have no effect).

The example~\ref{lst:subs}.b fails to make the recursive call in the first 
trail, because the subroutine is not awaiting event \code{g} in the moment its 
body calls itself.
Remember that events are unbuffered in \CEU.
The recursive call fails, but the subsequent call in the second trail behaves 
normally.

In example~\ref{lst:subs}.c, the first trail calls subroutine \code{h}, which 
awaits event \code{i} in its body.
Then, control returns to the second trail, which calls the subroutine again.
However, the subroutine did not complete and the second call is missed.

We show in Section~\ref{sec.adv.frp} that we even take advantage of 
non-recursive subroutines to properly describe mutual dependency between trails 
in parallel.

\begin{figure}[t]
%\rule{8.5cm}{0.37pt}
{\small
\begin{minipage}[t]{0.32\linewidth}
\begin{verbatim}
 event int f;
 par/or do
   loop do
     int v=await f;
     ...
   end
 with
   emit f(1);
   emit f(2);
 end


     (4.a)
   subroutine
\end{verbatim}
\end{minipage}
%
\hspace{0.5cm}
%
\begin{minipage}[t]{0.26\linewidth}
\begin{verbatim}
event void g;
par/or do
  loop do
    await g;
    ...
    emit g;
  end
with
  emit g;
  emit g;
end

    (4.b)
non-recursive
\end{verbatim}
\end{minipage}
\hspace{0.5cm}
%
\begin{minipage}[t]{0.25\linewidth}
\begin{verbatim}
event void h,i;
par/or do
  loop do
    await h;
    ...
    await i;
  end
with
  emit h;
  emit h;
end

    (4.c)
single instance
\end{verbatim}
\end{minipage}
\caption{ Subroutines in \CEU.
\label{lst:subs}
}
}
\end{figure}

\subsection{Finally blocks}
\label{sec.adv.fin}

\emph{Finally blocks} (as found in $Java$ and $C\#$) are often useful to handle 
dynamic resource allocation in a structured way.
As an example, the naive program in \CEU that follows allocates a block of 
memory and uses it across reactions to events before freeing it:

{\small
\begin{verbatim}
    input void A,F;
    par/or do
        _t* ptr = _malloc(...);
        ... // use `ptr'
        await A;
        ... // use `ptr' again
        _free(ptr);
    with
        await F;
    end
    ...     // program continues
\end{verbatim}
}

In the code, if event \code{F} occurs before \code{A}, the \code{par/or} 
composition terminates and does not free the allocated memory, leading to a 
leak in the program.

\CEU provides a \DOFIN construct to ensure the execution of a block of code to 
safely release resources.
The previous example can be rewritten as the code in the left side of
Figure~\ref{lst:finally}, which forces the execution of the block after the 
\FIN keyword, even when the outer \code{par/or} terminates.

\DOFIN constructs do not add any complexity to the semantics of \CEU, relying 
only on the set of primitives already presented in 
Section~\ref{sec.sem.syntax}.
For instance, the example is translated at compile time into the code shown in 
the right side of the figure, as follows:

\begin{enumerate}
\item A unique global internal event \code{\$fin} is declared.%
\footnote{Each \DOFIN is associated to an unique event (e.g.,  \code{\$fin\_1}, 
\code{\$fin\_2}, etc.).}
\item The \DOFIN is converted into a \code{par/and}.
\item The first \code{par/and} trail emits \code{\$fin} on termination to 
invoke the \FIN block.
\item The second \code{par/and} trail (the \FIN block) awaits \code{\$fin} to 
start executing.
\item All trails that terminate a \code{par/or} or escape a \code{loop} emit 
\code{\$fin} to also invoke the \FIN block.
\end{enumerate}

We opted for a dedicated syntax given that the transformation is not 
self-contained, affecting the global structure of programs.

The cases that follow illustrate the precise behavior of \FIN blocks when a 
third trail in parallel encloses a \DOFIN construct and kills it:

\begin{itemize}
\item 3rd trail terminates before the \FIN block starts to execute.
In this case, 3rd trail emits the corresponding \code{\$fin}, which is not yet 
being awaited for, and the \FIN does not execute.
\item 3rd trail terminates while the \DOFIN is blocked.
In this case, the resource has been acquired but not released.
The corresponding \code{\$fin} is emitted and holds 3rd trail to awake the 
\FIN, which safely releases the resource before resuming the terminating trail.
\item 3rd trail terminates concurrently with the \FIN (suppose they react to 
the same event).
In this case, both trails emit \code{\$fin}, executing the \FIN only once, as 
expected.
\end{itemize}

\begin{figure}[t]
%\rule{8.5cm}{0.37pt}
{\small
\begin{minipage}[t]{0.45\linewidth}
\begin{alltt}
  input void A,F;

  par/or do
    \textbf{do}
      \_t* ptr = \_malloc();
      ... // use `ptr'
      await A;
      ... // use `ptr' again

    \textbf{finally}

      \_free(ptr);
    \textbf{end}
  with
    await F;

  end
\end{alltt}
\end{minipage}
%
\hspace{0.5cm}
%
\begin{minipage}[t]{0.45\linewidth}
\begin{alltt}
input void A,F;
\textbf{event void \$fin;}      (1)
par/or do
  \textbf{par/and do}          (2)
    \_t* ptr = \_malloc();
    ... // use `ptr'
    await A;
    ... // use `ptr' again
    \textbf{emit \$fin;}        (3)
  \textbf{with}
    \textbf{await \$fin};       (4)
    \_free(ptr);
  \textbf{end}
with
  await F;
  \textbf{emit \$fin;}          (5)
end
\end{alltt}
\end{minipage}

\caption{ \DOFIN code and corresponding translation.
\label{lst:finally}
}
}
\end{figure}

\DOFIN constructs have the restriction that they cannot await events, otherwise 
they would be killed by the terminating trail before releasing the acquired 
resources.
However, releasing resources does not typically involve awaiting.

\subsection{Exception handling}
\label{sec.adv.excpt}

Exception handling can be provided by specialized programming language 
constructs (e.g., \code{try-catch} blocks in Java), but also with techniques 
using standard control-flow primitives (e.g., \code{setjmp/longjmp} in $C$).
\CEU can naturally express different forms of exception handling without a 
specific construct.

As an illustrative example, suppose an external entity periodically writes to a 
log file and notifies the program through the event \code{ENTRY}, which carries 
the number of available characters.

We start from a simple and straightforward specification to handle log entries 
assuming no errors occur.
The normal flow is to open the file and wait in a loop for \code{ENTRY} 
occurrences.
In our implementation, the low-level file operations \code{open} and 
\code{read} are accessed through associated internal events working as 
subroutines:

{\small
\begin{verbatim}
 // DECLARATIONS
 input int ENTRY;  // callback event for log entries
 _FILE*    f;      // holds a reference to the log
 char[10] buf;     // holds the current log entry
 event char* open; // opens filename into `f'
 event int read;   // reads a number of bytes into `buf'
 event int excpt;  // callback event for exceptions

 // NORMAL FLOW
 do
   emit open("log.txt");
   loop do
     int n = await ENTRY;
     emit read(n);              // reads into global `buf'
     _printf("log: %s\n", buf); // handles the log string
   end
 finally
   if f != _NULL then
     _fclose(f);
   end
 end
\end{verbatim}
}

We safely close the file in a \FIN block if the normal flow is killed by a 
surrounding block, as discussed in Section~\ref{sec.adv.fin}.

Emits to \code{open} and \code{read} behave just like conventional subroutines, 
relying on the stacked execution of internal events.
%A \code{read} operation does not await external events, hence, the program 
%loops and awaits event \code{ENTRY} continuously, reacting to all log writes.
The operations that perform the actual low-level system calls are placed in 
parallel and possibly emit exceptions through event \code{excpt}:

{\small
\begin{verbatim}
 // DECLARATIONS (as in previous code)
 par/or do
     // NORMAL FLOW (as in previous code)
 with
     loop do     // OPEN subroutine
         char* filename = await open;
         f = _open(filename);
         if f == _NULL then
             emit excpt(1);  // 1 = open exception
         end
     end
 with
     loop do     // READ subroutine
         int n = await read;
         if (n > 10) || (_read(f,buf,n) != n) then
             emit excpt(2);  // 2 = read exception
         end
     end
 end
\end{verbatim}
}

To handle exceptions, we enclose the normal flow with another \code{par/or} to 
terminate it on any exception thrown by file operations:

{\small
\begin{verbatim}
 // DECLARATIONS
 par/or do
     par/or do
         // NORMAL FLOW
     with
         await excpt;    // catch exceptions
     end
 with
     // OPERATIONS       // throw exceptions
 end
\end{verbatim}
}

\begin{comment}
Note that killing the normal behavior may yield a memory leak if `f' is open.
We can include a \DOFIN construct to enforce closing the file safely:
{\small
\begin{verbatim}
 ...
     par/or do
        do
            // NORMAL FLOW
        finally do
            if f != _NULL then
                _fclose(f);
            end
        end
     with
        await excpt;    // catch exceptions
     end
 ...
\end{verbatim}
}
\end{comment}

To illustrate how the program behaves on an exception, suppose the normal flow 
tries to read a string and fails.
The program behaves as follows (with the stack in emphasis):

{\small
\begin{enumerate}
\setlength{\itemsep}{0pt}
\item Normal flow invokes the read operation (\code{emit read}) and pauses;\\
    \emph{stack: [norm]}
\item Read operation awakes, throws an exception (\code{emit excpt}), and 
    pauses;\\
    \emph{stack: [norm, read]}
\item Exception handler (\code{await excpt}) awakes, invokes the \FIN (through 
    implicit \code{emit \$fin}), and pauses;\\
    \emph{stack: [norm, read, hdlr]}
\item The \FIN block executes, closes the file, and terminates;\\
    \emph{stack: [norm, read, hdlr]}
\item The exception continuation terminates the \code{par/or}, cancelling all 
remaining delayed continuations.\\
    \emph{stack: []}
\end{enumerate}
}

Exceptions in \CEU can also be recoverable if the handler does not terminate 
its surrounding \code{par/or}.
For instance, the new handler that follows waits for exceptions in a loop and 
recovers from each type of exception:

{\small
\begin{verbatim}
 ...
     par/or do
         // NORMAL FLOW
     with
         loop do
             int err = await excpt;  // catch exceptions
             if err == 1 then        // open exception
                 f = <creates a new file>
             else/if err == 2 then   // read exception
                 buf = <assigns a default string>
             end
         end
     end
 ...
\end{verbatim}
}

Now, step 3 in the previous execution trace would not fire the \FIN block, but 
instead, assign a default string to \code{buf}, loop and await the next 
exception.
Then, the exception continuation would loop and await further file operations.
In the end, the read operation would resume as if no exceptions had occurred.

Note that throughout the example, the normal flow remained unchanged, with all 
machinery to handle exceptions placed around it.
%The only modification was actually a bug removal, as we included the 
%\code{do-finally} block to ensure closing the file safely.

In terms of memory usage, switching from the original normal flow (without 
exception throws) to the last example (with recovery) incurred extra 450 bytes 
of ROM and 24 bytes of RAM.

The presented approach for exceptions has the limitation that file operations 
and exception handlers cannot await other events (related to the 
single-instance property of subroutines in \CEU).

\subsection{Dataflow programming}
\label{sec.adv.frp}

Reactive dataflow programming \cite{frp.survey} provides a declarative style to 
express dependency relationships among data.
A known issue in dataflow languages is on handling mutual dependency, which 
requires the explicit placement of a specific operator to avoid runtime
cycles~\cite{frtime.embedding,luagravity.sblp}.
This solution is somewhat \emph{ad hoc} and splits an internal dependency 
problem across two reactions to the environment.
%It also requires the mutual dependency to eventually converge to a value so 
%that variables do not affect each other forever.

\CEU can naturally express safe mutual dependencies, making it impossible to 
implement recursive definitions (as proved in 
Section~\ref{sec.safety.bounded}).
The program in \CEU in Figure~\ref{lst:ceu:frp:2} applies the temperature 
conversion formula between Celsius and Fahrenheit~\cite{frp.survey}, so that 
whenever the value in one unit is set, the other is automatically recalculated.

\begin{figure}[t]
%\rule{8.5cm}{0.37pt}
{\small
\begin{verbatim}
 1:   int tc, tf;
 2:   event int tc_evt, tf_evt;
 3:   par/or do
 4:      loop do             // 1st trail
 5:         tc = await tc_evt;
 6:         emit tf_evt(9 * tc / 5 + 32);
 7:      end
 8:   with
 9:      loop do             // 2nd trail
10:         tf = await tf_evt;
11:         emit tc_evt(5 * (tf-32) / 9);
12:      end
13:  with
14:      emit tc_evt(0);     // 3rd trail
15:      emit tf_evt(100);
16:  end
\end{verbatim}
}
\caption{ A dataflow program with mutual dependency.
\label{lst:ceu:frp:2}
}
\end{figure}

We first define the variables to hold the temperatures and corresponding 
internal events (lines 1-2).
Any change to a variable in the program must be signalled by an emit on the 
corresponding event so that dependent variables can react.
Then, we create two trails to await for changes and update the dependency 
relations among the temperatures.
For instance, the first trail is a \code{loop} (lines 4-7) that waits for 
changes on \code{tc\_evt} (line 5) and signals the conversion formula to 
\code{tf\_evt} (line 6).
The behavior for the second trail that awaits \code{tf\_evt} (lines 9-12) is 
analogous.
The third trail (lines 14-15) updates the temperatures twice in sequence.
The program behaves as follows (with the stack in emphasis):

{\small
\begin{enumerate}
\setlength{\itemsep}{0pt}
\item 1st and 2nd trail await \code{tc\_evt} and \code{tf\_evt};\\
    \emph{stack: []}
\item 3rd trail signals a change to \code{tc\_evt} and pauses;\\
    \emph{stack: [3rd]}
\item 1st trail awakes, sets \code{tc=0}, emits \code{tf\_evt}, and pauses;\\
    \emph{stack: [3rd,1st]}
\item 2nd trail awakes, sets \code{tf=32}, emits \code{tc\_evt}, and pauses;\\
    \emph{stack: [3rd,1st,2nd]}
\item no trails are awaiting \code{tc\_evt} (1st trail is paused), so 2nd trail 
    (on top of the stack) resumes, loops, and awaits \code{tf\_evt} again;\\
    \emph{stack: [3rd,1st]}
\item 1st trail resumes, loops, and awaits \code{tc\_evt} again;\\
    \emph{stack: [3rd]}
\item 3rd trail resumes and now signals a change to \code{tf\_evt};\\
    \emph{stack: [3rd]}
\item ... (analogous behavior)
\end{enumerate}
}

%It also shows that programs may trigger multiple reaction in sequence, within 
%the same reaction chain.
%For instance, when 3rd trail invokes \code{emit tf\_evt(100)} (line 15, step 
%7), the trails in parallel are already awaiting \code{tc\_evt} and 
%\code{tf\_evt} again (steps 5,6); hence, they will react again during the same 
%reaction chain (step 8 on).

%Altough xxx, no support for dynamic reconfiguration given the static nature of

%\newpage
%\section{Implementation of \CEU}

%\newpage
\section{Related work}
\label{sec.related}

\begin{comment}
TODO:
The static analysis precludes the dynamic creation of lines of execution, which 
are all statically ...
However, this trade-off seems to be favorable in the context of embedded 
systems, as dynamic features are discouraged due to resource limitations and 
safety requirements.
\end{comment}

\CEU is strongly influenced by Esterel~\cite{esterel.ieee91}, but they are 
different in the fundamental aspect of dealing with events (signals in 
Esterel).
\CEU's stacked execution for internal events greatly improves the 
expressiveness of the language as shown in Section~\ref{sec.adv}.
%For instance, dataflow programming is not feasible in Esterel, what resulted 
%in dedicated languages for that purpose~\cite{rp.twelve}.

Furthermore, Esterel is commonly used in hardware design, and its notion of 
time is similar to that of digital circuits, where multiple signals can be 
active at a clock tick.
In \CEU, instead of clock ticks, occurrences of external events define time 
units.
We believe that for software design, this approach simplifies the reasoning 
about concurrency.
For instance, the uniqueness of external events is a prerequisite for the 
static analysis that enables safe shared-memory concurrency in \CEU.

More recently, Wireless Sensor Networks (WSNs) emerged as an active research 
area for highly constrained embedded concurrency, resulting in the development 
of many synchronous languages~\cite{wsn.protothreads,wsn.sol,wsn.osm}.

Protothreads \cite{wsn.protothreads} offer lightweight cooperative 
multithreading for embedded systems.
Its stackless implementation reduces memory consumption but precludes support 
for local variables.
\CEU also avoids the use stacks for trails, but preserves support for locals 
by calculating the required memory at compile time.
%, as shown in Section~\ref{sec.safety.mem}.

SOL~\cite{wsn.sol} and OSM~\cite{wsn.osm} provide parallel state machines for 
WSNs, offering a formal and mature model for programming embedded systems.
However, the main contributions of \CEU, stacked execution for internal 
events and safe support for shared-memory concurrency, do not directly adapt to 
the state machines formalism.

In common among the referred works for WSNs is the agreement in providing 
low-level access for tasks (e.g., systems calls and shared-memory) through 
lock-free concurrency that precludes race conditions on programs.
However, they do not propose a reliable strategy for concurrent tasks accessing 
shared resources, as quoted from the references:

{\small
\begin{itemize}
\item \emph{The protothreads mechanism does not specify any specific method to 
invoke or schedule a protothread, this is defined by the system using 
protothreads.}~\cite{wsn.protothreads}
\item \emph{A single write access will always completely execute before the 
next write access can occur. However, the order in which write accesses are 
executed is arbitrary.}~\cite{wsn.osm}
\item \emph{The parallel operator executes all its threads in a round-robin 
manner according to the order of their declaration in the 
program.}~\cite{wsn.sol}
\end{itemize}
}

%Regarding the last policy, we believe that our proposed static analysis is an 
%improvement over deterministic schedulers.

On the opposite side of concurrent designs, asynchronous languages for embedded 
systems~\cite{wsn.mantisos,arduino.occam}
assume time independence among processes and are more appropriate for 
applications with a low synchronization rate or for those involving
algorithmic-intensive problems.
The described techniques for \DOFIN constructs and exception handling heavily 
rely on \code{par/or} compositions, which cannot be precisely defined in 
asynchronous languages without tweaking processes with synchronization 
mechanisms~\cite{esterel.preemption}.

\begin{comment}
SHIM is an asynchronous language that enforces synchronous communications among 
processes, providing a deterministic execution model.
SHIM distinguishes from typical asynchronous languages given that
The use of point-to-point communication, typical in CSP-like 
languages~\cite{async.csp}, leads to a different programming mindset.
No shared-memory
no hierarchies (e.g., \code{par/or} compositions)
\end{comment}

%TODO: limitation static (mantis, proto, occam tb!)

% TODO: leds, buffer overflows

Asynchronous models are also employed in real-time operating systems to provide 
response predictability, typically through prioritized 
schedulers~\cite{wsn.mantisos,wsn.survey,freertos}.
Even though \CEU ensures bounded execution for reactions, it cannot provide 
hard real-time warranties.
For instance, assigning different priorities for trails would break lock-free 
concurrency (i.e., breaking correctness is worse than breaking timeliness).
%synchronous model and
%the static analysis, which are required for

%That said, some embedded systems do require prioritized scheduling to meet 
%deadlines for critical tasks, even if it involves extra complexity to deal 
%%with synchronization issues.
Fortunately, \CEU and RTOSes are not mutually exclusive, and we can foresee a 
scenario in which multiple \CEU programs run in different RTOS threads and 
communicate asynchronously via external events, an architecture known as GALS 
(\emph{globally asynchronous--locally synchronous})~\cite{rp.gals}.

\section{Conclusion}
\label{sec.conclusion}

\begin{comment}

A design that achieves a high degree of reliability while embracing practical 
aspects such as low-level access.

most bug are in control aspects
spaghetti code
not on C calls

Main contribution, stacked execution,
fundamental for the bounded execution (used on proof)
and for deriving control primitives

TODO: evaluation
- compared to handcrafted...

unifies imperative and dataflow synchronous languages

instead of hiding shared-memory, as proposed by cite{bocchino lee},
we sticked to a different conc. model which enabled..

no restrictions of current works
    locals, fine-grained conc, bounded, deterministic
stackless
    still locals

This behavior, which we consider to be the expected one for emits in sequence, 
is naturally achieved with the stack execution policy for internal events.

\appendix
\section{Appendix Title}

\subsection{Bounded execution}

\newtheorem{mydef}{Definition}
\begin{mydef}
Here is a new definition
\end{mydef}
In order to identify tight loops in program, we define function $noLoop$ as 
follows:

{\small
\begin{align*}
noLoop(~await~e~) &= (true,~true,~true)                             \\
noLoop(~break~)   &= (true,~true,~false)                            \\
noLoop(~p~;~q~)   &= (ok1 \wedge ok2,~stp1 \vee stp2,~awt1 \vee awt2)  \\
                   &   \ten where~(ok1,stp1,awt1)=noLoop(p)          \\
                   &   \ten\5 and~(ok2,stp2,awt2)=noLoop(q)          \\
noLoop(~p~and~q~)&= (ok1 \wedge ok2,~stp1 \wedge stp2,~awt1 \wedge awt2) \\
   |\5~~~(~p~or~q~) &   \ten where~(ok1,stp1,awt1)=noLoop(p)            \\
|~(~if~..~~p~..~q~) & \ten\5 and~(ok2,stp2,awt2)=noLoop(q)    \\
noLoop(~loop~p~)  &= (ok \wedge stp,~awt,~awt)                  \\
                   &   \ten where~(ok,stp,awt)=noLoop(p)             \\
noLoop(*)         &= (true,~false,~false) \5 (i.e~mem,emit)          %\\
\end{align*}
}

The function manipulates triples of predicates $(ok,stp,awt)$ referring to 
subtrees in a program:
$ok$ indicates no presence of a tight loop,
$stp$ indicates the presence of either an \code{await} or \code{break}, and
$awt$ indicates the presence of an \code{await}.

The definition states that once a tight loop occurs in any path, it is 
propagated to parents.
In sequences, $stp$ and $awt$ predicates use the disjunction operator ($\vee$) 
between $p$ and $q$, because if either of them await or break, the composition 
also does.
However, for conditionals and parallel primitives, the program does not 
necessarily goes through $p$ and $q$, hence rules use conjunctions ($\wedge$) 
over the predicates.
A $loop$ is considered tight when its body neither awaits or breaks 
($stp=false$).
Inner breaks do not affect outer loops, hence predicate $stp$ becomes $awt$ as 
breaks are ``consumed'' by the loop.

At first sight, an $and$ could use a less restrictive definition, such as the 
one used for sequences.
However, in the presented semantics, a program such as \code{(loop (emit a and 
await a))} executes forever, because the delayed \code{emit} and the 
\code{await} would always match in the big step, yielding a tight loop.
%We opted for clarity and kept the restrictive definition, instead of having to 
%handle corner cases.

Now, given that programs with tight loops are refused at compile time, we can 
prove that a reaction chain always executes in bounded time,
i.e, that the presented semantics always make a finite number of derivations to 
reach the terminating conditions of Section~\ref{sec.sem}.

The proof is by structural induction on derivation trees of programs.
Any tree either blocks or terminates in a finite number of transformations.

Starting from the small-step rules, as base cases, the primary primitives
$nop$ and $break$ terminate in zero steps;
$mem$ terminates in one step (rule \textbf{mem});
$emit$ blocks in one step (rule \textbf{emit});
and $await$ and $delay$ block in zero steps.
As recursive cases,
an $if$ is reduced in one step to one of its branches which we can apply the 
inductive hypothesis;
an unblocked $sequence$ is reduced to either, otherwise it must be blocked.
 over them.


Rule \textbf{loop-expd} is the only one that augments the program, possibly 
making a reaction chain to run forever.
However, programs do not contain tight loops, i.e., their bodies either await 
or break.
If a loop

If a loop breaks, 

All other small-step rules lead to a condition where , either (1)consume, 
(2)await, (3)terminate

Unblocked programs necessarily block because all unblocked primitives
The only rule that
We prove this statement by structural induction on derivation rules of the 
semantics presented in Section~\ref{} (TODO):

Base cases:
$$
p = nop
p = break
mem -> nop
$$

As base cases $nop$ and $break$ terminate in zero derivations.

%- p -> p' e isBlocked(p')
%- p' --> p'' e nao cresce

A big step always makes a transition, either by not matching
We prove this claim by structural induction on the structure of a program.
The
(a menos de MEM)
primeiro provar que ST termina
nop can'advance
await can't advance (ST)
emit -> nop
mem  -> nop

\acks

\end{comment}

\begin{comment}

Acknowledgments, if needed.
\end{comment}

%\newpage
\bibliographystyle{abbrvnat}
\bibliography{other,my}

\begin{comment}
% INTRO
Embedded systems are essentially reactive, as they must interact at the speed 
of the surrounding environment through I/O devices.
Usually they are designed with safety and real-time requirements, and are 
constrained in memory and processing.
%Unfortunatelly, this dillema two demands are unlikely to be compatible.
As a consequence, language designs for embedded systems either remove 
facilities to reduce complexity, or fail to offer a small and reliable 
programming environment.
A static analysis that detects unbounded loops and nondeterministic access to 
shared memory at compile time safe shared-memory concurrency through static 
analysis,
, avoiding common
primitives calls
We believe that
that helps on ensuring the correctness of programs through static 
verification~\cite{rp.twelve}.
and provides two distinguished features:
\CEU supports concurrent lines of execution that run in time steps and are 
allowed to share variables.
simpler, restrictive, lock-free
control-intensive
time consuming computations
\CEU is grounded on a synchronous execution model, and provides two 
distinguished features:
safe shared-memory concurrency through static analysis, and an atypical stacked 
execution for internal events.
On top of this kernel, we show how to derive advanced control mechanisms, such 
as exception handling, finalizers, and dataflow programming.
We take advantage of this tractable model to detect unbounded loops and 
nondeterministic access to shared memory at compile time, avoiding common 
concurrency pitfalls in current language designs.
We present a formal description of \CEU, which is then used to demonstrate 
how our proposed safety warranties are legitimate.
\end{comment}

\begin{comment}
\begin{table}[t]\small
\begin{center}
\begin{tabular}{ | l | c | c | c | c | c | }
\hline
        & ID-rd & ID-wr & PTR-rd & PTR-wr & C-call \\
\hline
ID-rd   &   X   &       &    X   &        &   X  \\
\hline
ID-wr   &       &       &        &        &   X  \\
\hline
PTR-rd  &   X   &       &    X   &        &   X  \\
\hline
PTR-wr  &       &       &        &        &   X  \\
\hline
C-call  &   X   &   X   &    X   &    X   &      \\
\hline
\end{tabular}
\end{center}
\caption{Safe parallel access to memory.}
\label{tab:nd}
\end{table}

Table~\ref{tab:nd} summarizes the policy that classifies pairs of $mem$ 
operations marked in $X$ as safely executable in parallel.
\emph{ID-* vs ID-*} cells represent two accesses in parallel to the \emph{same} 
variable (different variables can be naturally accessed concurrently).
\emph{ID-* vs PTR-*} cells represent accesses to a given variable and to a 
pointer that contains the type of that variable (e.g., \code{int~a} vs 
\code{int*~b}).
$C$ calls never conflict with variable acesses.
\end{comment}

\end{document}
